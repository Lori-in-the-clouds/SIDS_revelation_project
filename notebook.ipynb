{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Selects the best available device for PyTorch computations",
   "id": "7386cf7bfc715121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:52:24.982792Z",
     "start_time": "2025-07-15T15:52:24.979219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "1e8d8bfe46b0a6c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 181
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Augmentation",
   "id": "c5798e7f9d0694"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code defines four image augmentation pipelines using Albumentations, tailored for YOLO-style object detection. Each pipeline applies a series of transformations—such as grayscale conversion, blurring, noise injection, brightness/contrast adjustment, affine transformations, and compression—while ensuring that bounding boxes remain valid. These augmentations simulate challenging visual conditions like poor lighting, motion blur, or camera noise, and are stored in a list (night_aug_list) for easy selection during dataset expansion or training.",
   "id": "d90c877f2a7e8f85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T16:21:43.438639Z",
     "start_time": "2025-07-15T16:21:43.420742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "bbox_params = A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.0, check_each_transform=True)\n",
    "\n",
    "filter_0 = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.4, -0.2), contrast_limit=(-0.3, 0), p=1.0),\n",
    "    A.MotionBlur(blur_limit=(5, 15), p=0.6),\n",
    "    A.GaussianBlur(blur_limit=(5, 11), p=0.4),\n",
    "    A.MedianBlur(blur_limit=7, p=0.3),\n",
    "    A.GaussNoise(p=0.6),\n",
    "    A.ISONoise(color_shift=(0.01, 0.03), intensity=(0.1, 0.4), p=0.3),\n",
    "    A.HueSaturationValue(hue_shift_limit=(-2, 2), sat_shift_limit=(-25, -10), val_shift_limit=(-10, 0), p=0.5),\n",
    "    A.Affine(rotate=(-90, 90), p=1)],\n",
    "    bbox_params= bbox_params\n",
    ")\n",
    "\n",
    "filter_1 = A.Compose([\n",
    "    A.ToGray(p=1.0),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=(-0.5, -0.3),  # scurisce ma non troppo\n",
    "        contrast_limit=(-0.3, -0.1),    # contrasto moderato\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.GaussianBlur(blur_limit=(5, 10), p=0.5),   # sfocatura media\n",
    "    A.ISONoise(color_shift=(0.01, 0.03), intensity=(0.15, 0.35), p=0.4),  # rumore ISO più leggero\n",
    "    A.SaltAndPepper(p=0.07),  # rumore salt and pepper più leggero\n",
    "    A.HueSaturationValue(hue_shift_limit=0, sat_shift_limit=(-5, -2), val_shift_limit=(-10, -5), p=0.4),\n",
    "    A.Affine(rotate=(-90, 90), p=1)\n",
    "], bbox_params=bbox_params)\n",
    "\n",
    "filter_2 = A.Compose([\n",
    "    A.ToGray(p=1.0),\n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "    A.ImageCompression(p=0.4),\n",
    "    A.MotionBlur(blur_limit=(3, 7), p=0.4),\n",
    "    A.GaussNoise(p=0.3),\n",
    "    A.Affine(rotate=(-90, 90), p=1)\n",
    "    ],\n",
    "    bbox_params= bbox_params\n",
    ")\n",
    "\n",
    "filter_3 = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, -0.05), contrast_limit=(-0.2, 0.05), p=1.0),\n",
    "    A.CLAHE(clip_limit=(1, 3), tile_grid_size=(8, 8), p=0.4),\n",
    "    A.Equalize(mode='cv', p=0.3),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.4),\n",
    "    A.ImageCompression(p=0.3),\n",
    "    A.Affine(translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)}, rotate=(-3, 3),p=0.3),\n",
    "    A.Affine(rotate=(-90, 90), p=1)\n",
    "    ],\n",
    "    bbox_params= bbox_params\n",
    ")\n",
    "\n",
    "filter_4 = A.Compose([\n",
    "    A.GaussianBlur(blur_limit=(21, 31), p=1.0),             # Sfocatura gaussiana massima\n",
    "    A.MotionBlur(blur_limit=(25, 35), p=0.9),               # Sfocatura da movimento forte\n",
    "    A.MedianBlur(blur_limit=15, p=0.7),                     # Sfocatura ulteriore\n",
    "    A.GaussNoise(p=0.7),           # Rumore intenso (opzionale)\n",
    "    A.ImageCompression( p=0.7),  # Compressione forte\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.0), contrast_limit=(-0.2, 0.05), p=0.8),\n",
    "    A.Affine(rotate=(-10, 10), p=0.6),                      # Piccole rotazioni\n",
    "    A.Affine(rotate=(-90, 90), p=1.0)                       # Rotazione YOLO obbligatoria\n",
    "], bbox_params=bbox_params)\n",
    "\n",
    "night_aug_list = [filter_0, filter_1,filter_2,filter_3,filter_4]"
   ],
   "id": "d7a2898e1d9213fb",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This function reads an image and its YOLO-format annotations, clips and normalizes the bounding boxes to ensure they remain within the [0.0, 1.0] range, then applies a selected augmentation from night_aug_list. The transformed image is saved, and the new bounding boxes are clipped again and written to the output label file, ensuring valid annotations after transformation.",
   "id": "78d057ed6a7c6dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:53:35.275249Z",
     "start_time": "2025-07-15T15:53:35.267927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_yolo_bbox(bbox, epsilon=1e-7):\n",
    "\n",
    "    cx, cy, w, h = bbox\n",
    "\n",
    "    # Arrotonda per ridurre le imprecisioni in virgola mobile\n",
    "    cx = round(cx, 7)\n",
    "    cy = round(cy, 7)\n",
    "    w = round(w, 7)\n",
    "    h = round(h, 7)\n",
    "\n",
    "    # Calcola i limiti min/max\n",
    "    x_min = cx - w / 2\n",
    "    y_min = cy - h / 2\n",
    "    x_max = cx + w / 2\n",
    "    y_max = cy + h / 2\n",
    "\n",
    "    # Clipa i limiti min/max all'intervallo [0.0, 1.0 - epsilon] per i massimi\n",
    "    x_min = max(0.0, x_min)\n",
    "    y_min = max(0.0, y_min)\n",
    "    x_max = min(1.0 - epsilon, x_max) # Usa epsilon per evitare 1.0 esatto\n",
    "    y_max = min(1.0 - epsilon, y_max) # Usa epsilon per evitare 1.0 esatto\n",
    "\n",
    "    # Ricostruisci il bounding box YOLO dal formato min/max clippato\n",
    "    new_cx = (x_min + x_max) / 2\n",
    "    new_cy = (y_min + y_max) / 2\n",
    "    new_w = x_max - x_min\n",
    "    new_h = y_max - y_min\n",
    "\n",
    "    # Assicurati che larghezza e altezza non siano negative o quasi zero\n",
    "    new_w = max(0.0, new_w)\n",
    "    new_h = max(0.0, new_h)\n",
    "\n",
    "    return [new_cx, new_cy, new_w, new_h]\n",
    "\n",
    "\n",
    "def apply_transformation(image_input, image_output, mode, label_input, label_output):\n",
    "    import cv2\n",
    "\n",
    "    image = cv2.imread(image_input)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    bboxes = []\n",
    "    class_labels = []\n",
    "    with open(label_input, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                class_id = int(parts[0])\n",
    "                bbox = list(map(float, parts[1:]))\n",
    "                # IMPORTANTE: Clipa le coordinate del bbox STRETTAMENTE prima di passarle ad Albumentations\n",
    "                # Questo assicura che tutti i valori siano nell'intervallo [0.0, 1.0] per prevenire ValueError\n",
    "                normalized_bbox = normalize_yolo_bbox(bbox)\n",
    "                bboxes.append(normalized_bbox)\n",
    "                class_labels.append(class_id)\n",
    "\n",
    "    augmented = night_aug_list[mode](\n",
    "        image=image,\n",
    "        bboxes=bboxes,\n",
    "        class_labels=class_labels\n",
    "    )\n",
    "\n",
    "    augmented_bgr = cv2.cvtColor(augmented['image'], cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(image_output, augmented_bgr)\n",
    "\n",
    "    clipped_bboxes = []\n",
    "    clipped_labels = []\n",
    "    for bbox, label in zip(augmented['bboxes'], augmented['class_labels']):\n",
    "        # Di nuovo clipping per sicurezza\n",
    "        bbox = [min(max(x, 0.0), 1.0) for x in bbox]\n",
    "        x, y, w, h = bbox\n",
    "        if w > 0.001 and h > 0.001:\n",
    "            clipped_bboxes.append([x, y, w, h])\n",
    "            clipped_labels.append(label)\n",
    "\n",
    "    with open(label_output, 'w') as f:\n",
    "        for label, bbox in zip(clipped_labels, clipped_bboxes):\n",
    "            bbox_str = \" \".join(f\"{v:.6f}\" for v in bbox)\n",
    "            f.write(f\"{label} {bbox_str}\\n\")"
   ],
   "id": "6808eb080b4b645b",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cleans previously augmented files and applies new augmentations to images and their labels in a dataset, saving the results with a suffix indicating the augmentation mode. It skips images missing labels.",
   "id": "ee0bf25fdd3554e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:53:38.656034Z",
     "start_time": "2025-07-15T15:53:38.650785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def clean_dataset(dir):\n",
    "\n",
    "    dir_images = Path(dir+\"/images\")\n",
    "    dir_labels = Path(dir+\"/labels\")\n",
    "\n",
    "    images = sorted(f for f in dir_images.iterdir() if f.is_file())\n",
    "    labels = sorted(f for f in dir_labels.iterdir() if f.is_file())\n",
    "\n",
    "    for image in images:\n",
    "        if re.search(r\"_t\\d+\\.jpg$\", image.name):\n",
    "                image.unlink(missing_ok=True)\n",
    "\n",
    "    for label in labels:\n",
    "        if label:\n",
    "            if re.search(r\"_t\\d+\\.txt$\", label.name):\n",
    "                label.unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "def add_items_in_dataset(dir, mode):\n",
    "    dir_images = Path(dir + \"/images\")\n",
    "    dir_labels = Path(dir + \"/labels\")\n",
    "\n",
    "    images = sorted(f for f in dir_images.iterdir() if f.is_file() and not re.search(r\"_t\\d+\\.jpg$\", f.name))\n",
    "\n",
    "    for image in images:\n",
    "        image_name = str(image).split(\".jpg\")[0]\n",
    "        label_path = Path(str(image_name).replace(\"/images/\", \"/labels/\") + \".txt\")\n",
    "\n",
    "        if not label_path.exists():\n",
    "            print(f\"⚠️ Label mancante per {image.name}, salto.\")\n",
    "            continue\n",
    "\n",
    "        image_output_path = str(image_name) + f\"_t{mode}.jpg\"\n",
    "        label_output_path = str(label_path).replace(\".txt\", f\"_t{mode}.txt\")\n",
    "\n",
    "        apply_transformation(\n",
    "            image_input=str(image),\n",
    "            label_input=str(label_path),\n",
    "            image_output=image_output_path,\n",
    "            label_output=label_output_path,\n",
    "            mode=mode\n",
    "        )"
   ],
   "id": "377e13ed77fa274a",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T16:02:14.925938Z",
     "start_time": "2025-07-15T16:02:14.851943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_all_dataset(dataset_path):\n",
    "    clean_dataset(str(dataset_path+\"/test\"))\n",
    "    clean_dataset(str(dataset_path+\"/train\"))\n",
    "    clean_dataset(str(dataset_path+\"/valid\"))\n",
    "\n",
    "clean_all_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8\")"
   ],
   "id": "58e06769d85836c7",
   "outputs": [],
   "execution_count": 192
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:58:26.931759Z",
     "start_time": "2025-07-15T15:53:46.845213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Data augmentation Test\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/test\",0)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/test\",1)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/test\",2)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/test\",3)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/test\",4)\n",
    "\n",
    "#Data augmentation Train\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/train\",0)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/train\",1)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/train\",2)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/train\",3)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/train\",4)\n",
    "\n",
    "#Data augmentation Test\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/valid\",0)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/valid\",1)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/valid\",2)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/valid\",3)\n",
    "add_items_in_dataset(\"/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/valid\",4)"
   ],
   "id": "868ba7d589af6b34",
   "outputs": [],
   "execution_count": 187
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Carica il modello pre-addestrato o uno vuoto\n",
    "model = YOLO('yolov8n.pt')  # modello nano pre-addestrato, puoi scegliere anche yolov8s.pt ecc.\n",
    "\n",
    "# Avvia il training\n",
    "model.train(data='/Users/lorenzodimaio/PyCharmMiscProject/second_try_face_detection.v1i.yolov8/data.yaml', epochs=1, imgsz=640, batch=16,device=device)"
   ],
   "id": "2ab1210a919d8b0f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
