{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Selects the best available device for PyTorch computations",
   "id": "7386cf7bfc715121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T08:49:11.580613Z",
     "start_time": "2025-09-01T08:49:10.997234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "1e8d8bfe46b0a6c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Augmentation",
   "id": "c5798e7f9d0694"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T08:49:12.237113Z",
     "start_time": "2025-09-01T08:49:11.632704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "\n",
    "bbox_params = A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.0, check_each_transform=True)\n",
    "\n",
    "night_filter_0 = A.Compose([\n",
    "    A.ToGray(p=0.7),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.4, -0.2), contrast_limit=(-0.2, 0.1), p=0.9),\n",
    "    A.GaussNoise(std_range=(0.039, 0.196), p=0.5),\n",
    "    A.MotionBlur(blur_limit=(3, 7), p=0.3),\n",
    "    A.ImageCompression(quality_range=(20,40),p=0.4),\n",
    "    A.Affine(rotate=(-5, 5), p=0.4),\n",
    "], bbox_params=bbox_params)\n",
    "\n",
    "night_filter_1 = A.Compose([\n",
    "    A.ToGray(p=1.0),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.5, -0.3), contrast_limit=(-0.3, -0.1), p=1.0),\n",
    "    A.GaussianBlur(blur_limit=(5, 9), p=0.5),\n",
    "    A.ISONoise(intensity=(0.2, 0.4), p=0.7),\n",
    "    A.ImageCompression(quality_range=(5,25), p=0.4),\n",
    "    A.Affine(rotate=(-10, 10), p=0.4)\n",
    "], bbox_params=bbox_params)\n",
    "\n",
    "night_filter_2 = A.Compose([\n",
    "    A.ToGray(p=0.9),\n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=0.7),\n",
    "    A.ISONoise(intensity=(0.15, 0.35), p=0.4),\n",
    "    A.MotionBlur(blur_limit=(3, 7), p=0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.3, -0.1), contrast_limit=(-0.2, 0.1), p=0.7),\n",
    "    A.ImageCompression(quality_range=(20,40),p=0.5),\n",
    "    A.Affine(rotate=(-10, 10), p=0.4)\n",
    "], bbox_params=bbox_params)\n",
    "\n",
    "night_filter_3 = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.0), contrast_limit=(-0.2, 0.05), p=1.0),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.4),\n",
    "    A.ImageCompression(quality_range=(30,50),p=0.4),\n",
    "    A.Defocus(radius=(3, 5), alias_blur=True, p=0.3),\n",
    "    A.Affine(rotate=(-5, 5), p=0.3)\n",
    "], bbox_params=bbox_params)\n",
    "\n",
    "night_filter_4 = A.Compose([\n",
    "    A.ToGray(p=1.0),\n",
    "    A.ISONoise(intensity=(0.2, 0.4), p=0.5),\n",
    "    A.MotionBlur(blur_limit=(5, 11), p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.4, -0.1), contrast_limit=(-0.2, 0.1), p=0.8),\n",
    "    A.Affine(rotate=(-90, 90), p=1.0)  # YOLO rotation obbligatoria\n",
    "], bbox_params=bbox_params)\n",
    "\n",
    "night_filter_5 = A.Compose([\n",
    "    A.ToGray(p=1.0),\n",
    "    A.Downscale(scale_range=(0.3, 0.6), p=0.6),\n",
    "    A.GaussNoise(std_range=(0.05, 0.2), p=0.5),\n",
    "    A.ImageCompression(quality_range=(1, 30), p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.3, 0), contrast_limit=(-0.2, 0.1), p=0.7),\n",
    "    A.Defocus(radius=(3, 5), alias_blur=True, p=0.3),\n",
    "    A.Affine(\n",
    "        translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n",
    "        rotate=(-5, 5),\n",
    "        scale=(0.9, 1.1),\n",
    "        p=0.3\n",
    "    ),\n",
    "], bbox_params=bbox_params)\n",
    "\n",
    "night_filter_6 = A.Compose([\n",
    "    A.ToGray(p=0.8),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.4, -0.2), contrast_limit=(-0.3, -0.1), p=1.0),\n",
    "    A.CoarseDropout(num_holes_range=(1,3),hole_height_range=(30,60),hole_width_range=(30,60),fill=0,p=0.5),\n",
    "    A.MotionBlur(blur_limit=(3, 7), p=0.3),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.2),\n",
    "    A.Affine(rotate=(-10, 10), p=0.3)\n",
    "], bbox_params=bbox_params)\n",
    "\n",
    "night_aug_list = [night_filter_0, night_filter_1, night_filter_2, night_filter_3, night_filter_4,night_filter_5, night_filter_6]"
   ],
   "id": "d7a2898e1d9213fb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T08:49:12.249005Z",
     "start_time": "2025-09-01T08:49:12.240992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def add_items_in_dataset(dir_path, mode):\n",
    "    dir_images = Path(dir_path) / \"images\"\n",
    "    dir_labels = Path(dir_path) / \"labels\"\n",
    "\n",
    "    images = sorted(\n",
    "        f for f in dir_images.iterdir()\n",
    "        if f.is_file() and not re.search(r\"_t\\d+\\.jpg$\", f.name)\n",
    "    )\n",
    "\n",
    "    for image in images:\n",
    "        image_stem = image.stem\n",
    "        label_path = dir_labels / f\"{image_stem}.txt\"\n",
    "\n",
    "        if not label_path.exists():\n",
    "            print(f\"⚠️ Label mancante per {image.name}, salto.\")\n",
    "            continue\n",
    "\n",
    "        image_output = dir_images / f\"{image_stem}_t{mode}.jpg\"\n",
    "        label_output = dir_labels / f\"{image_stem}_t{mode}.txt\"\n",
    "\n",
    "        try:\n",
    "            apply_transformation(\n",
    "                image_input=str(image),\n",
    "                image_output=str(image_output),\n",
    "                label_input=str(label_path),\n",
    "                label_output=str(label_output),\n",
    "                mode=mode\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Errore su {image.name}: {e}\")\n",
    "\n",
    "\n",
    "def normalize_yolo_bbox(bbox, epsilon=1e-7):\n",
    "\n",
    "    cx, cy, w, h = bbox\n",
    "\n",
    "    cx = round(cx, 7)\n",
    "    cy = round(cy, 7)\n",
    "    w = round(w, 7)\n",
    "    h = round(h, 7)\n",
    "\n",
    "    x_min = cx - w / 2\n",
    "    y_min = cy - h / 2\n",
    "    x_max = cx + w / 2\n",
    "    y_max = cy + h / 2\n",
    "\n",
    "    x_min = max(0.0, x_min)\n",
    "    y_min = max(0.0, y_min)\n",
    "    x_max = min(1.0 - epsilon, x_max)\n",
    "    y_max = min(1.0 - epsilon, y_max)\n",
    "\n",
    "    new_cx = (x_min + x_max) / 2\n",
    "    new_cy = (y_min + y_max) / 2\n",
    "    new_w = x_max - x_min\n",
    "    new_h = y_max - y_min\n",
    "\n",
    "    new_w = max(0.0, new_w)\n",
    "    new_h = max(0.0, new_h)\n",
    "\n",
    "    return [new_cx, new_cy, new_w, new_h]\n",
    "\n",
    "\n",
    "def apply_transformation(image_input, image_output, mode, label_input, label_output):\n",
    "    import cv2\n",
    "\n",
    "    image = cv2.imread(image_input)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    bboxes = []\n",
    "    class_labels = []\n",
    "    with open(label_input, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                class_id = int(parts[0])\n",
    "                bbox = list(map(float, parts[1:]))\n",
    "\n",
    "                normalized_bbox = normalize_yolo_bbox(bbox)\n",
    "                bboxes.append(normalized_bbox)\n",
    "                class_labels.append(class_id)\n",
    "\n",
    "    augmented = night_aug_list[mode](\n",
    "        image=image,\n",
    "        bboxes=bboxes,\n",
    "        class_labels=class_labels\n",
    "    )\n",
    "\n",
    "    assert len(augmented['bboxes']) == len(augmented['class_labels']), \\\n",
    "        f\"bbox-label length mismatch: {len(augmented['bboxes'])} vs {len(augmented['class_labels'])}\"\n",
    "\n",
    "    augmented_bgr = cv2.cvtColor(augmented['image'], cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(image_output, augmented_bgr)\n",
    "\n",
    "    clipped_bboxes = []\n",
    "    clipped_labels = []\n",
    "    for bbox, label in zip(augmented['bboxes'], augmented['class_labels']):\n",
    "        bbox = [min(max(x, 0.0), 1.0) for x in bbox]\n",
    "        x, y, w, h = bbox\n",
    "        if w > 0.001 and h > 0.001:\n",
    "            clipped_bboxes.append([x, y, w, h])\n",
    "            clipped_labels.append(label)\n",
    "\n",
    "    with open(label_output, 'w', encoding='utf-8') as f:\n",
    "        for label, bbox in zip(clipped_labels, clipped_bboxes):\n",
    "            bbox_str = \" \".join(f\"{v:.6f}\" for v in bbox)\n",
    "            f.write(f\"{label} {bbox_str}\\n\")\n",
    "\n",
    "def clean_dataset(dir_path):\n",
    "\n",
    "    dir_images = Path(dir_path + \"/images\")\n",
    "    dir_labels = Path(dir_path + \"/labels\")\n",
    "\n",
    "    images = sorted(f for f in dir_images.iterdir() if f.is_file())\n",
    "    labels = sorted(f for f in dir_labels.iterdir() if f.is_file())\n",
    "\n",
    "    for image in images:\n",
    "        if re.search(r\"_t\\d+\\.jpg$\", image.name):\n",
    "                image.unlink(missing_ok=True)\n",
    "\n",
    "    for label in labels:\n",
    "        if label:\n",
    "            if re.search(r\"_t\\d+\\.txt$\", label.name):\n",
    "                label.unlink(missing_ok=True)"
   ],
   "id": "e9e6f94754f9a98c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T08:49:12.256818Z",
     "start_time": "2025-09-01T08:49:12.254878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_data_augmentation(dataset_path):\n",
    "    clean_dataset(str(dataset_path+\"/test\"))\n",
    "    clean_dataset(str(dataset_path+\"/train\"))\n",
    "    clean_dataset(str(dataset_path+\"/valid\"))\n",
    "\n",
    "    for i in range(0,7):\n",
    "        add_items_in_dataset(str(dataset_path+\"/test\"),i)\n",
    "        add_items_in_dataset(str(dataset_path+\"/train\"),i)\n",
    "        add_items_in_dataset(str(dataset_path+\"/valid\"),i)"
   ],
   "id": "6511b103221fcfba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exectute data Augmentation",
   "id": "99b09eed0591a3a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T08:50:04.892661Z",
     "start_time": "2025-09-01T08:49:47.177250Z"
    }
   },
   "cell_type": "code",
   "source": "apply_data_augmentation(\"/Users/lorenzodimaio/PyCharmMiscProject/Dataset_v3.v1i.yolov8\")",
   "id": "ccab833594c9f7b1",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mapply_data_augmentation\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/Users/lorenzodimaio/PyCharmMiscProject/Dataset_v3.v1i.yolov8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[4], line 8\u001B[0m, in \u001B[0;36mapply_data_augmentation\u001B[0;34m(dataset_path)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m7\u001B[39m):\n\u001B[1;32m      7\u001B[0m     add_items_in_dataset(\u001B[38;5;28mstr\u001B[39m(dataset_path\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/test\u001B[39m\u001B[38;5;124m\"\u001B[39m),i)\n\u001B[0;32m----> 8\u001B[0m     \u001B[43madd_items_in_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataset_path\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/train\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     add_items_in_dataset(\u001B[38;5;28mstr\u001B[39m(dataset_path\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/valid\u001B[39m\u001B[38;5;124m\"\u001B[39m),i)\n",
      "Cell \u001B[0;32mIn[3], line 25\u001B[0m, in \u001B[0;36madd_items_in_dataset\u001B[0;34m(dir_path, mode)\u001B[0m\n\u001B[1;32m     22\u001B[0m label_output \u001B[38;5;241m=\u001B[39m dir_labels \u001B[38;5;241m/\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage_stem\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_t\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 25\u001B[0m     \u001B[43mapply_transformation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimage_output\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlabel_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlabel_output\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m❌ Errore su \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 85\u001B[0m, in \u001B[0;36mapply_transformation\u001B[0;34m(image_input, image_output, mode, label_input, label_output)\u001B[0m\n\u001B[1;32m     82\u001B[0m             bboxes\u001B[38;5;241m.\u001B[39mappend(normalized_bbox)\n\u001B[1;32m     83\u001B[0m             class_labels\u001B[38;5;241m.\u001B[39mappend(class_id)\n\u001B[0;32m---> 85\u001B[0m augmented \u001B[38;5;241m=\u001B[39m \u001B[43mnight_aug_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbboxes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbboxes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_labels\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(augmented[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbboxes\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(augmented[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclass_labels\u001B[39m\u001B[38;5;124m'\u001B[39m]), \\\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbbox-label length mismatch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(augmented[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbboxes\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m vs \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(augmented[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclass_labels\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     94\u001B[0m augmented_bgr \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(augmented[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m], cv2\u001B[38;5;241m.\u001B[39mCOLOR_RGB2BGR)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/yolov8_env/lib/python3.10/site-packages/albumentations/core/composition.py:612\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, force_apply, *args, **data)\u001B[0m\n\u001B[1;32m    610\u001B[0m     data \u001B[38;5;241m=\u001B[39m t(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[1;32m    611\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_track_transform_params(t, data)\n\u001B[0;32m--> 612\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_data_post_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(data)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/yolov8_env/lib/python3.10/site-packages/albumentations/core/composition.py:382\u001B[0m, in \u001B[0;36mBaseCompose.check_data_post_transform\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    377\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data_name, data_value \u001B[38;5;129;01min\u001B[39;00m data\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    378\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m data_name \u001B[38;5;129;01min\u001B[39;00m proc\u001B[38;5;241m.\u001B[39mdata_fields \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m    379\u001B[0m                 data_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_additional_targets\n\u001B[1;32m    380\u001B[0m                 \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_additional_targets[data_name] \u001B[38;5;129;01min\u001B[39;00m proc\u001B[38;5;241m.\u001B[39mdata_fields\n\u001B[1;32m    381\u001B[0m             ):\n\u001B[0;32m--> 382\u001B[0m                 data[data_name] \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m/opt/anaconda3/envs/yolov8_env/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:272\u001B[0m, in \u001B[0;36mBboxProcessor.filter\u001B[0;34m(self, data, shape)\u001B[0m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Filter bounding boxes based on size and visibility criteria.\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    269\u001B[0m \n\u001B[1;32m    270\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams: BboxParams\n\u001B[0;32m--> 272\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfilter_bboxes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmin_area\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin_area\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmin_visibility\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin_visibility\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmin_width\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin_width\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmin_height\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin_height\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_accept_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_accept_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/yolov8_env/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:710\u001B[0m, in \u001B[0;36mfilter_bboxes\u001B[0;34m(bboxes, shape, min_area, min_visibility, min_width, min_height, max_accept_ratio)\u001B[0m\n\u001B[1;32m    707\u001B[0m denormalized_box_areas \u001B[38;5;241m=\u001B[39m calculate_bbox_areas_in_pixels(bboxes, shape)\n\u001B[1;32m    709\u001B[0m \u001B[38;5;66;03m# Clip bounding boxes in ratio\u001B[39;00m\n\u001B[0;32m--> 710\u001B[0m clipped_bboxes \u001B[38;5;241m=\u001B[39m \u001B[43mclip_bboxes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbboxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    712\u001B[0m \u001B[38;5;66;03m# Calculate areas of clipped bounding boxes in pixels\u001B[39;00m\n\u001B[1;32m    713\u001B[0m clipped_box_areas \u001B[38;5;241m=\u001B[39m calculate_bbox_areas_in_pixels(clipped_bboxes, shape)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/yolov8_env/lib/python3.10/site-packages/albumentations/augmentations/utils.py:245\u001B[0m, in \u001B[0;36mhandle_empty_array.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(array) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m array\n\u001B[0;32m--> 245\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/yolov8_env/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:670\u001B[0m, in \u001B[0;36mclip_bboxes\u001B[0;34m(bboxes, shape)\u001B[0m\n\u001B[1;32m    667\u001B[0m denorm_bboxes[:, [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m]] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(denorm_bboxes[:, [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m]], \u001B[38;5;241m0\u001B[39m, height, out\u001B[38;5;241m=\u001B[39mdenorm_bboxes[:, [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m]])\n\u001B[1;32m    669\u001B[0m \u001B[38;5;66;03m# Normalize clipped bboxes\u001B[39;00m\n\u001B[0;32m--> 670\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnormalize_bboxes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdenorm_bboxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/yolov8_env/lib/python3.10/site-packages/albumentations/augmentations/utils.py:245\u001B[0m, in \u001B[0;36mhandle_empty_array.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(array) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m array\n\u001B[0;32m--> 245\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "c0f51c4942ed01bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Carica il modello pre-addestrato o uno vuoto\n",
    "model = YOLO('yolov8n.pt')  # modello nano pre-addestrato, puoi scegliere anche yolov8s.pt ecc.\n",
    "\n",
    "# Avvia il training\n",
    "model.train(data='/Users/lorenzodimaio/PyCharmMiscProject/Dataset_v3.v1i.yolov8/data.yaml', epochs=1, imgsz=320, batch=16,device=device)"
   ],
   "id": "2ab1210a919d8b0f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
